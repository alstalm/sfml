{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# прочитаем train_2_col.csv\n",
    "train_4_col_2 = pd.read_csv('train_4_col_2.csv', sep='\\t', index_col = 0)\n",
    "train_4_col_2_small = train_4_col_2.head(nrows)\n",
    "train_4_col_2_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# прочитаем train_4_col_3 .csv\n",
    "train_4_col_3 = pd.read_csv('train_4_col_3.csv', sep='\\t', index_col = 0)\n",
    "train_4_col_3_small = train_4_col_3.head(nrows)\n",
    "train_4_col_3_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# прочитаем train_4_col.csv\n",
    "train_4_col = pd.read_csv('train_4_col.csv', sep='\\t', index_col = 0)\n",
    "train_4_col_small = train_4_col.head(nrows)\n",
    "train_4_col_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Объединим маленькие датафреймы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"3\">Внимание!!!.</font><br> \n",
    "\n",
    "чтобы объединились полные датафреймы - надо убрать окончание \"_small\" в ячейке ниже. Далее будет использовано название train. После объединения полных датафреймов это название менять не надо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train_4_col_3.merge(train_4_col_2, on='id', how='left').merge(train_4_col, on='id', how='left')\n",
    "train.head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция, которая извлекает значений ключей из поля \"fields\"\n",
    "def valextractor(field, key1=0, key2='name'):\n",
    "    try:\n",
    "        value = eval(field[1:-1])[key1]['field'][key2]         \n",
    "    except SyntaxError:\n",
    "        value = ''\n",
    "    except KeyError:\n",
    "        value = ''\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# проверяем функцию. работает.\n",
    "field = train.iloc[0,1]\n",
    "valextractor(field, key1=0,key2='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# функция, которая доавляет новые категории\n",
    "def getcat(df):\n",
    "    df_ = df.copy()\n",
    "    df_['fd_name1'] = df_['fields'].map(lambda x: (valextractor(x, key1=0,key2='name')))\n",
    "    df_['fd_name2'] = df_['fields'].map(lambda x: (valextractor(x, key1=1,key2='name')))\n",
    "    df_['fd_id1'] = df_['fields'].map(lambda x: (valextractor(x, key1=0,key2='id')))\n",
    "    df_['fd_id2'] = df_['fields'].map(lambda x: (valextractor(x, key1=1,key2='id')))\n",
    "    df_['fd_slug1'] = df_['fields'].map(lambda x: (valextractor(x, key1=0,key2='slug')))\n",
    "    df_['fd_slug2'] = df_['fields'].map(lambda x: (valextractor(x, key1=1,key2='slug')))\n",
    "    df_['fd_slug_id1'] = df_['fields'].map(lambda x: (valextractor(x, key1=0,key2='slug_id')))\n",
    "    df_['fd_slug_id2'] = df_['fields'].map(lambda x: (valextractor(x, key1=1,key2='slug_id')))\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# применим функцию\n",
    "train = train.pipe(getcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# удалим задублированные столбцы\n",
    "train.drop(columns=['fd_id1','fd_id2','fields' ],axis=1, inplace=True)\n",
    "# посмотрим что получилось\n",
    "train.head(50).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведем морфологический анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2 # Морфологический анализатор.\n",
    "#from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph=pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь. теперь переменная morph может проводить морфологический анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMeaningfullWords(text):\n",
    "    words=[]\n",
    "    tokens=re.findall('[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+', text)\n",
    "    for t in tokens:\n",
    "        pv=morph.parse(t)\n",
    "        for p in pv:\n",
    "            if p.tag.POS in ['ADJF', 'NOUN', 'VERB']:\n",
    "                words.append(p.normal_form)  # Хочу получить все значимые слова, но в уже нормальной форме\n",
    "                break\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_rows',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант 1. Создадим столбец 'morph_description' - где будет описание, прошедшее морфологический анализ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# пропустим этот вариант\n",
    "#%%time\n",
    "#train['morph_description'] = train['description'].fillna('').astype('object')\\\n",
    "#.map(lambda text: ' '.join([morph.parse(r)[0].normal_form for r in re.findall('[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+', text)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# посмотрим что получилось\n",
    "#train.loc[5000,'morph_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант 2. Посмотрим что получится при другом варианте морфологического анализа - тот же столбец 'morph_description'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train['morph_description'] = train['description'].fillna('').astype('object')\\\n",
    ".map(lambda text: ' '.join(getMeaningfullWords(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.loc[3,'morph_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем такой же с функцией **getMeaningfullWords** \"морфологический текст\" для столбца \"name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# уже знакомая функция, но для названий: с английскими буквами и цифрами\n",
    "def getMeaningfullWords_forname(text):\n",
    "    words=[]\n",
    "    tokens=re.findall('[А-Яа-яЁё0-9]+\\-[А-Яа-яЁё0-9]+|[А-Яа-яЁё0-9]+|[A-Za-z0-9]+', text)\n",
    "    for t in tokens:\n",
    "        pv=morph.parse(t)\n",
    "        for p in pv:\n",
    "            if p.tag.POS in ['NOUN' ]:           \n",
    "                words.append(p.normal_form)  # Хочу получить все значимые слова, но в уже нормальной форме\n",
    "            break\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_['morph_name_2'] = train_['morph_name'].fillna('').astype('object')\\\n",
    ".map(lambda text: ' '.join(getMeaningfullWords_forname(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 9003\n",
    "print('Название ДО морфологического анализа    :',train.loc[n,'name'])\n",
    "print('Название ПОСЛЕ морфологического анализа :',train.loc[n,'morph_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получим списки с частотами по Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = train['morph_description'] # Создадим корпус значимых слов\n",
    "n = 3\n",
    "example = train.loc[n,'morph_description'] # получаем n-ю строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfCounter=TfidfVectorizer(ngram_range=(1,2), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "analyze = tfCounter.build_analyzer()\n",
    "res=tfCounter.fit_transform(corpus) # Скармливаем сюда корпус значимых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res2=analyze(example) # подаем ранее пролученную n-ю строку\n",
    "\n",
    "# Получим значения частот всех слов n-й строки\n",
    "tfs=list(set(res[n][0, tfCounter.vocabulary_.get(k)] for k in res2 if k in tfCounter.vocabulary_.keys())) # укажем номер n \n",
    "\n",
    "# получим значения частот только тех слов, значения которых превышают среднее значение частот n-й строки\n",
    "tfs2=[k for k in tfs if k>np.average(tfs)]\n",
    "\n",
    "# или получим значения частот только тех слов, значения которых превышают среднее значение + Стандартное отклонение \n",
    "# частот n-й строки\n",
    "#tfs2=[k for k in tfs if k>np.average(tfs)+np.std(tfs)]\n",
    "\n",
    "print({w:res[n][0, tfCounter.vocabulary_[w]] for \\\n",
    "          w in res2 if w in tfCounter.vocabulary_.keys() and\\\n",
    "       res[n][0, tfCounter.vocabulary_[w]] in tfs2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или  вариант со средним значением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "example = train.loc[n,'morph_description'] # получаем строку\n",
    "\n",
    "res2=analyze(example) # подаем строку\n",
    "\n",
    "\n",
    "tfs=list(set(res[n][0, tfCounter.vocabulary_.get(k)] for k in res2 if k in tfCounter.vocabulary_.keys()))\n",
    "\n",
    "#tfs2=[k for k in tfs if k>np.average(tfs)]\n",
    "tfs2=[k for k in tfs if k>np.average(tfs)+np.std(tfs)]\n",
    "\n",
    "print({w:res[n][0, tfCounter.vocabulary_[w]] for \\\n",
    "          w in res2 if w in tfCounter.vocabulary_.keys() and\\\n",
    "       res[n][0, tfCounter.vocabulary_[w]] in tfs2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно это еще пригодится. Вставить это в новый столбец, старое описание удалить, запустить dictVectorizer - , должно получиться покомпактнее, возможно будет и результат получше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Просто почитать описания\n",
    "row = 31\n",
    "train.loc[row,'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Просто посмотреть список значимых слов\n",
    "train_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраним, прочитаем train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train.to_csv('train_merged_full_morphed_2.csv')\n",
    "train = pd.read_csv('train_merged_full_morphed_2.csv', index_col=0)\n",
    "#train_.drop(columns=['fd_name1','fd_name2','fd_slug_id1','fd_slug_id2',\\\n",
    "#                    'fd_slug1','fd_slug2','description','name','Unnamed: 0' ],axis=1, inplace=True) #07.01.19\n",
    "\n",
    "\n",
    "#train_['morph_description'] = train['morph_description'].fillna('').astype('object')\n",
    "train['morph_name_2'] = train['morph_name_2'].fillna('').astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimirpolanski6/anaconda3/lib/python3.5/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "#train.to_csv('train_merged_full_morphed_2_light.csv')\n",
    "train = pd.read_csv('train_merged_full_morphed_2_light.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategory</th>\n",
       "      <th>morph_name_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>914</td>\n",
       "      <td>сумка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2202</td>\n",
       "      <td>комплект</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subcategory morph_name_2\n",
       "0          914        сумка\n",
       "1         2202     комплект"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поробуем Tf-Idf в \"чистом\" виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество стоп-слов составляет: 27\n"
     ]
    }
   ],
   "source": [
    "# Задаим стоп слова\n",
    "stop_words1 = ['век','год','месяц','день','час','минута','секунда','иза','продать','купить','звонить','цена','влюбиться',\n",
    "'состояние','дорг','размер','танк','продавать','сантиметр', 'брать','быть','пользование','понадобиться','любой',\n",
    "'рубль','контакт','продаваться']\n",
    "print('Количество стоп-слов составляет:',len(stop_words1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество стоп-слов составляет: 97\n"
     ]
    }
   ],
   "source": [
    "# Задаим стоп слова для второго векторайзера\n",
    "stop_words2 = [               \n",
    "   'новое','цена','флисовыя','кожа','зам','набор','танец','кузнецов','мальчик', 'новое', 'новый', 'новая', 'метр', 'год', 'вт',  \n",
    "  'дюйм', 'литр',  'рубль', 'дым', '100р' ,'28шт', 'мл', '1' ,'2','3', '4', '7', '8', '9', '10', '11', '12', '13', '14', '15', \n",
    "    '16', '17', '18', '19', 'rx', 'сутки', 'вренда', 'рубль', 'ps3', 'superslim', 's', 'торг',  'фламинго', 'g', '104плюс', \n",
    "    '110куб', '12-18мес','128г', '1970г', '1год', 'магаз', 'орига', '2007год', '2008г', '2008год', '200р', '2010г', '2016г', \n",
    "    '21-22р', '2-4г', 'двойнить', '3000игра',  '5500р', '5копейка',  'состояние', '7000-шт', '76см', '80см', '22-36кг', 'бронь',\n",
    "    'стиль', 'сша', 'рост', 'обмен', 'литр','2003год', '2004г', '2003г', 'цвет', '62-64р', 'метр', '1500мма',\n",
    "    '18мес', '21', '22', '22р', '36кг', '4г', '62', '64р', '7000', 'шт'\n",
    "]\n",
    "print('Количество стоп-слов составляет:',len(stop_words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "Tfidf_array = vectorizer.fit_transform(train['morph_description'])\n",
    "#print('Список слов, определяющих частотный список:',vectorizer.get_feature_names())\n",
    "print('Размеры (shape) полученного массива Tfidf_array (строк, слов):', Tfidf_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Так можно получить матрицу частот - полежит пока здесь, на всякий случай..\n",
    "#pd.set_option('display.max_rows', 500) \n",
    "#df_tfidf = pd.DataFrame(Tfidf_array.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Зададим селектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X_, y=None): # \n",
    "        return self\n",
    "    def transform(self, X_):\n",
    "        return X_.loc[:, self.attribute_names].values # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DenseTransformer(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        #return X.todense()\n",
    "        return X.toarray()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим списки столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_cat_columns : ['subcategory']\n",
      "tfidf_columns   : ['morph_name_2']\n",
      "Остаток         : []\n"
     ]
    }
   ],
   "source": [
    "# получим список столбцов \n",
    "full_columns_list = list(train) \n",
    "\n",
    "# список числовых категориальных столбцов\n",
    "num_cat_columns = ['subcategory']\n",
    "\n",
    "tfidf_columns = ['morph_name_2']\n",
    "\n",
    "# целвое значение\n",
    "target = ['price']\n",
    "print('num_cat_columns :',num_cat_columns)\n",
    "print('tfidf_columns   :',tfidf_columns)\n",
    "\n",
    "other = list(set(full_columns_list) - set(num_cat_columns)  - set(tfidf_columns) - set(target))\n",
    "print('Остаток         :', other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\" size=\"3\">Pipline 1.</font><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([  \n",
    "('selector_2', ColumnSelector(num_cat_columns)), # определим категориальные числовые столбцы: ['subcategory']\n",
    "('OHE', OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimirpolanski6/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1748890 entries, 0 to 1748889\n",
      "Columns: 250 entries, 0 to 249\n",
      "dtypes: float64(250)\n",
      "memory usage: 3.3 GB\n"
     ]
    }
   ],
   "source": [
    "# Это для проверки. Не запускать\n",
    "model_categories = cat_pipeline.fit_transform\n",
    "categories_array = model_categories(train)\n",
    "df_categories_array = pd.DataFrame(categories_array.toarray())\n",
    "df_categories_array.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Это для проверки. Не запускать\n",
    "df_concat = pd.concat([train,df_categories_array], axis = 1)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\" size=\"3\">Pipline 2.</font><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_pipeline = Pipeline([  \n",
    "('selector', ColumnSelector(tfidf_columns[0])), # определим столбцы для  tfidf  преобразования: ['morph_name_2']\n",
    "('Tf*Idf', TfidfVectorizer(max_features = 4000,  stop_words=stop_words2)), #  max_features = 2000, max_df=0.95, min_df =0.05,\n",
    "('dense',DenseTransformer()),\n",
    "('p1' , PCA(n_components = 10)) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#train['morph_name_2'] = train['morph_name_2'].fillna('').astype('object')\n",
    "\n",
    "tfidf_pipeline.fit(train.iloc[:500000,:]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tfidf_pipeline_10param.pickle', 'wb') as f:\n",
    "     pickle.dump(tfidf_pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tfidf_pipeline.pickle', 'rb') as f:\n",
    "     tfidf_pipeline = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "step = 5000\n",
    "max_value = len(train) #.iloc[:100000,:]\n",
    "cols = 10\n",
    "X_pca_full_10params = np.zeros((0, cols))\n",
    "\n",
    "for i in range(0,max_value,step):\n",
    "    X_pca_i = tfidf_pipeline.transform(train.iloc[i:(i+step),:]) #.iloc[:500000,:]\n",
    "    X_pca_full_10params = np.vstack((X_pca_full_10params, X_pca_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pca_full_10params_df = pd.DataFrame(X_pca_full_10params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladimirpolanski6/anaconda3/lib/python3.5/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# train_10params.to_csv('train_10params')\n",
    "train_10params = pd.read_csv('train_10params.csv', index_col=0)\n",
    "X = train_10params.drop(['category','price','morph_name_2'], axis = 1)\n",
    "y = train_10params['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>price</th>\n",
       "      <th>morph_name_2</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>914</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>сумка</td>\n",
       "      <td>-0.049054</td>\n",
       "      <td>-0.048199</td>\n",
       "      <td>-0.008001</td>\n",
       "      <td>-0.111666</td>\n",
       "      <td>-0.213300</td>\n",
       "      <td>0.952322</td>\n",
       "      <td>0.018150</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.069384</td>\n",
       "      <td>0.034727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>2202</td>\n",
       "      <td>350.0</td>\n",
       "      <td>комплект</td>\n",
       "      <td>-0.036504</td>\n",
       "      <td>-0.019080</td>\n",
       "      <td>-0.003647</td>\n",
       "      <td>-0.019831</td>\n",
       "      <td>-0.014673</td>\n",
       "      <td>-0.018714</td>\n",
       "      <td>-0.006571</td>\n",
       "      <td>-0.002524</td>\n",
       "      <td>-0.026301</td>\n",
       "      <td>-0.023684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  subcategory   price morph_name_2         0         1         2  \\\n",
       "0         9          914  1999.0        сумка -0.049054 -0.048199 -0.008001   \n",
       "1        22         2202   350.0     комплект -0.036504 -0.019080 -0.003647   \n",
       "\n",
       "          3         4         5         6         7         8         9  \n",
       "0 -0.111666 -0.213300  0.952322  0.018150  0.004751  0.069384  0.034727  \n",
       "1 -0.019831 -0.014673 -0.018714 -0.006571 -0.002524 -0.026301 -0.023684  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_10params.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('X_pca_full_10params.pickle', 'wb') as f:\n",
    "     pickle.dump(X_pca_full_10params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('X_pca_full_p2.pickle', 'wb') as f:\n",
    "     pickle.dump(X_pca_full, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</font><br>  <font color=\"magenta\" size=\"5\">CatBoost.</font><br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "catboost_model = CatBoostRegressor(depth = 6, num_trees=700, learning_rate = 0.11,cat_features=[0], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got unsafe target value = 2.73e+06 at object #1380 of dataset learn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7fefa71998d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catboost_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "catboost_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CatBoost Вариант БЕЗ PCA\n",
    "#model_2= Pipeline([('f', featureunion_pipeline), ('x', catbost_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_2.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чистый CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "catbost_model.fit(X_small, np.log(y_small+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_small.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('catbost_model.pickle', 'wb') as f:\n",
    "     pickle.dump(catbost_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('catbost_model.pickle', 'rb') as f:\n",
    "     catbost_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.drop(['0_x','1_x','2_x','3_x','0_y','1_y', '2_y', '3_y'], axis = 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_te = X.iloc[1000000:1700000,:]\n",
    "y_te = y.iloc[1000000:1700000]\n",
    "print(X_te.shape)\n",
    "print(y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat_log = catbost_model.predict(X_te)\n",
    "y_hat_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = np.exp(y_hat_log)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mean_absolute_error(y_te,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train__ = train_.iloc[1000000:1700000,:] # возьмем только предсказанную часть трейна\n",
    "train__.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_te_df = pd.DataFrame(y_te)\n",
    "y_hat_df = pd.DataFrame(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hat_df.index = range(1000000,1700000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# объединим предсказанную часть трейна, цену и предсказанную цену\n",
    "train_predicted = pd.concat([train__, y_hat_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_predicted.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_te.shape)\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузим и подготовим тестовые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategory</th>\n",
       "      <th>morph_name_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>стремянка</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>плита</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>диск</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1009</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1104</td>\n",
       "      <td>ледобур рыбалка</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subcategory     morph_name_2\n",
       "0          603        стремянка\n",
       "1          203            плита\n",
       "2          116             диск\n",
       "3         1009                 \n",
       "4         1104  ледобур рыбалка"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test_merged_full_morphed.csv', index_col= 0)\n",
    "#test.to_csv('test_merged_full_morphed.csv')\n",
    "test['morph_name_2'] = test['morph_name_2'] .fillna('').astype('object')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "step = 5000\n",
    "max_value = len(test) #.iloc[:100000,:]\n",
    "cols = 10\n",
    "test_pca_full_10params = np.zeros((0, cols))\n",
    "\n",
    "for i in range(0,max_value,step):\n",
    "    X_pca_i = tfidf_pipeline.transform(test.iloc[i:(i+step),:]) #.iloc[:500000,:]\n",
    "    test_pca_full_10params = np.vstack((test_pca_full_10params, X_pca_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_pca_full_10params_df = pd.DataFrame(test_pca_full_10params)\n",
    "test_pca_full_10params_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_.csv', sep = \";\", index_col=0)\n",
    "#test_pca_full_10params_df.to_csv('test_pca_full_10params_df.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = catboost_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('Исходники/submit_Sample.csv') # прочитаем шаблон\n",
    "sample['price'] = prediction # Запишем прогноз\n",
    "sample['price'] = sample['price'].map(abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample.to_csv(\"_submission_2.csv\", index = False) # Запишем в файл"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
